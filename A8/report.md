---
title: "DL Lab8: Semantic Segmentation"
author: "313832008 簡蔚驊"
CJKmainfont: "Noto Sans CJK TC"
fontsize: 12pt
geometry: "a4paper, top=0.6cm, bottom=1.4cm, left=0.8cm, right=0.8cm"
numbersections: true
toc: true
---

# 摘要

本次實驗旨在實作一個基於 U-Net 的語意分割模型，使用 PyTorch 框架針對 CCAgT 數據集進行訓練、測試與評估。CCAgT 數據集包含高分辨率的組織切片影像，每張影像經 AgNOR 技術染色，具有多種分類標籤，適合語意分割的應用場景。實驗過程中，我們對數據集進行了分割與預處理，將其劃分為訓練集、驗證集和測試集，並通過多種評估指標（如 Pixel Accuracy、mIoU、Dice Coefficient 和 mAP）對模型進行性能測試與比較。此外，我們使用多種可視化方式來呈現語意分割結果，展示模型的分割能力和不足之處。本實驗最終驗證了 U-Net 模型在多分類語意分割任務中的有效性，並總結了實驗過程中的挑戰與可能的改進方向。

# 背景

語意分割是深度學習中的重要應用之一，其目標是為影像中的每個像素賦予語意標籤。這種技術廣泛應用於醫學影像分析、遙感影像處理、自動駕駛及其他需要像素級別精確分類的領域。在醫學影像分析中，語意分割特別有助於區分不同組織或病灶，提供診斷支持。

CCAgT 數據集是針對組織切片影像設計的公開數據集，由高分辨率影像和相應的像素標籤組成，每個像素表示影像中的不同分類。每張影像的分辨率為 512x512 像素，且包含至少一個語意標籤。這些影像經過 AgNOR 染色技術處理，能夠揭示不同組織的結構特徵。

本實驗選擇 U-Net 模型作為主要架構，這是一種專為語意分割設計的全卷積神經網絡，其通過編碼器提取特徵並解碼器進行重建，配合跳躍連接保留高層次與低層次特徵信息，能夠有效處理高分辨率、多分類的語意分割任務。實驗還將探索如何優化模型訓練過程，包括數據增強技術、損失函數選擇和指標評估，旨在提升模型的準確性與穩定性。

# 模型架構

本次實驗採用 U-Net 模型進行語意分割，該模型以全卷積神經網絡（Fully Convolutional Network, FCN）為基礎，專為處理像素級別的分割任務設計，具有結構簡單、特徵提取精確和效率高的優勢。以下將詳細說明模型的架構設計，包括編碼器、解碼器以及跳躍連接的設計。

## 編碼器（Encoder）

編碼器的主要功能是提取影像的多層次特徵，逐步減少影像的空間分辨率，保留更高層次的語意信息。該部分由四層結構組成，每層包含：

- 卷積層：兩個 3×3 的卷積操作，用於特徵提取。
- 激活函數：使用 SiLU（Sigmoid-Weighted Linear Unit），其平滑非線性特性能提高模型在小數據集上的表現。
- Batch Normalization：穩定訓練過程，提升收斂速度。
- 池化層：2×2 的 Max Pooling，將特徵圖的空間大小縮小為原來的一半。

| 編碼層 | 輸入特徵圖大小 | 輸出特徵圖大小 | 卷積核數量 |
|-------|---------------|---------------|------------|
| Input | 512×512×3     | 512×512×32    | 32         |
| Down1 | 512×512×32    | 256×256×64    | 64         |
| Down2 | 256×256×64    | 128×128×128   | 128        |
| Down3 | 128×128×128   | 64×64×256     | 256        |
| Down4 | 64×64×256     | 32×32×512     | 512        |

## 解碼器（Decoder）

解碼器的功能是通過逐步恢復影像的空間分辨率，將提取的高層次特徵還原至輸出的像素標籤圖。其結構包含：

- 反卷積層：使用 2×2 的轉置卷積（Transposed Convolution），將特徵圖大小上採樣為原來的兩倍。
- 卷積層：與編碼器類似，包含兩個 3×3 的卷積操作。
- 激活函數：同樣使用 SiLU。
- Batch Normalization：穩定解碼過程。

| 解碼層 | 輸入特徵圖大小 | 輸出特徵圖大小 | 卷積核數量 |
|-------|---------------|---------------|------------|
| Up1   | 32×32×512     | 64×64×256     | 256        |
| Up2   | 64×64×256     | 128×128×128   | 128        |
| Up3   | 128×128×128   | 256×256×64    | 64         |
| Up4   | 256×256×64    | 512×512×32    | 32         |
| Output| 512×512×32    | 512×512×8     | 8（分類數）|

## 跳躍連接（Skip Connections）

U-Net 的創新在於跳躍連接設計，將編碼器中每一層的輸出與解碼器中相應層的輸入拼接。這種方式有效保留了影像的低層次細節特徵，並與解碼器中的高層次特徵融合，提升模型對邊界細節的處理能力。跳躍連接的特點包括：
- 裁剪操作：為了匹配特徵圖大小，對編碼層的特徵圖進行裁剪後再拼接。
- 特徵融合：將高分辨率的低層次特徵與低分辨率的高層次特徵結合，提高分割的精細度。

## 像素級別分割

U-Net 通過逐層恢復影像分辨率，最終使用 1×1 的卷積層生成輸出圖，每個像素的深度對應於分類數（本次實驗為 8）。模型通過對每個像素的分類，實現精確的語意分割。

# 實驗方法

## 數據預處理

- 本實驗使用 CCAgT 數據集，其中包含 9339 張高分辨率影像，每張影像對應多分類標籤。
- 按照 7:1:2 的比例將數據集劃分為訓練集、驗證集和測試集，確保各部分數據分布一致。
  - 訓練集（70%）：用於模型參數的訓練。
  - 驗證集（10%）：用於模型的性能監控及超參數調整。
  - 測試集（20%）：用於最終模型性能的評估。

## 模型訓練

### 超參數設置

- 學習率（Learning Rate, LR）：初始學習率設為 \(1 \times 10^{-4}\)。
- 批次大小（Batch Size）：設定為 12。
- 訓練輪數（Epochs）：設定為 30，確保模型有足夠的訓練次數來收斂。
- 隨機種子：設置 `torch.manual_seed(12)` 和 `torch.backends.cudnn.deterministic = True` 以保證結果可重現。

### 優化器與學習率調度器

- 優化器（Optimizer）：使用 Adam 優化器，其動態學習率更新特性能有效加速收斂。
- 學習率調度器（Scheduler）：採用 Cosine Annealing Scheduler，逐步降低學習率以獲得更好的泛化性能。

### 訓練過程

- 損失函數：採用交叉熵損失函數（CrossEntropy Loss）計算像素級別的分類誤差。
- 逐步累積梯度（Gradient Accumulation）：為了緩解顯存限制，實現了梯度累積（每 4 個批次更新一次梯度）。
- 驗證：每輪訓練後在驗證集上計算損失，記錄模型的性能變化曲線。

# 結果分析與比較

## 模型性能

以下表格匯總了在測試數據集上的模型評估結果，包括主要指標 Pixel Accuracy、Class Pixel Accuracy、Mean Pixel Accuracy、Dice Coefficient、IoU 和 mAP：

| 指標名稱               | 數值                        |
|------------------------|-----------------------------|
| Pixel Accuracy     | **99.28%**                 |
| Mean Pixel Accuracy| 76.49%                 |
| Mean IoU           | 0.680                  |
| mAP@[.5:.95:.05]   | 0.031                  |

各類別性能指標：

| 類別      | Class Pixel Accuracy | Dice Coefficient | IoU   |
|-----------|-----------------------|------------------|-------|
| 類別 0    | 99.71%               | 0.998            | 0.995 |
| 類別 1    | **90.65%**               | 0.909            | 0.833 |
| 類別 2    | 81.01%               | 0.736            | 0.582 |
| 類別 3    | 58.21%               | 0.380            | 0.234 |
| 類別 4    | 88.92%               | 0.868            | 0.766 |
| 類別 5    | 87.80%               | 0.874            | 0.777 |
| 類別 6    | 79.58%               | 0.763            | 0.617 |
| 類別 7    | 74.91%               | 0.775            | 0.633 |

## 結果可視化

選取語意分割效果幾個案例進行展示，包括：

![結果展示](./A8/assets/result.png){ width=80% }

## 訓練過程

訓練與驗證損失的變化曲線顯示如下圖所示：

訓練與驗證損失曲線顯示模型成功收斂，整體表現良好，未來可透過正則化或數據增強進一步提升泛化能力。

![損失曲線](./A8/assets/loss.png){ width=60% }

# 總結

本實驗成功實作了基於 U-Net 的語意分割模型，並在 CCAgT 數據集上進行了系統化的訓練與測試。模型在 Pixel Accuracy 和部分類別的 Class Pixel Accuracy 上表現出色。此外，通過可視化分析展示了模型的分割能力及不足之處。未來可嘗試數據增強技術和損失函數優化，提升對低頻類別的識別能力，並探索更先進的模型架構來進一步提高分割性能。